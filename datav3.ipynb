{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import json\n",
    "import yfinance as yf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import ARIMA\n",
    "from arch import arch_model\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "import shap\n",
    "import warnings\n",
    "import zipfile\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Configure settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# FRED availability (optional)\n",
    "try:\n",
    "    from fredapi import Fred\n",
    "    _HAS_FREDAPI = True\n",
    "except Exception:\n",
    "    _HAS_FREDAPI = False\n",
    "\n",
    "FRED_API_KEY = os.getenv(\"FRED_API_KEY\")\n",
    "if not FRED_API_KEY:\n",
    "    raise RuntimeError(\n",
    "        \"FRED_API_KEY not found\"\n",
    "    )\n",
    "\n",
    "if _HAS_FREDAPI:\n",
    "    fred = Fred(api_key=FRED_API_KEY)\n",
    "    \n",
    "EIA_API_KEY = os.getenv(\"EIA_API_KEY\")\n",
    "if not EIA_API_KEY:\n",
    "    raise RuntimeError(\n",
    "        \"EIA_API_KEY not found in environment.\"\n",
    "    )\n",
    "\n",
    "# optional holiday calendar for release-day shifts\n",
    "try:\n",
    "    import holidays\n",
    "    US_HOLIDAYS = holidays.UnitedStates()\n",
    "except Exception:\n",
    "    US_HOLIDAYS = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions â€” Market ingestion, geopolitics, GPR/EPU/BDI, weather, analysis helpers\n",
    "\n",
    "All helper functions from your original notebook are included below (unchanged)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data_tickers():\n",
    "    \"\"\"\n",
    "    Ingests raw time series data. Downloads daily data from Yahoo Finance.\n",
    "\n",
    "    Includes:\n",
    "    - WTI (CL=F), Brent (BZ=F), Gold (GC=F), Copper (HG=F)\n",
    "    - DXY (DX-Y.NYB), 10Y Treasury (^TNX), VIX (^VIX)\n",
    "    - S&P 500 (^GSPC) and realized volatility (20D rolling, annualized)\n",
    "    - 3M Treasury Bill (^IRX) as short-term rate proxy\n",
    "    - NASDAQ (^IXIC)\n",
    "    \"\"\"\n",
    "\n",
    "    # Define tickers and column mapping\n",
    "    tickers = {\n",
    "        'CL=F': 'wti_price',          # WTI Crude Oil Futures\n",
    "        'BZ=F': 'brent_price',        # Brent Crude Oil Futures\n",
    "        'GC=F': 'gold_price',         # Gold Futures\n",
    "        'HG=F': 'copper_price',       # Copper Futures\n",
    "        'DX-Y.NYB': 'dxy',            # US Dollar Index\n",
    "        '^TNX': '10y_yield',          # 10-Year Treasury Yield (percent)\n",
    "        '^VIX': 'vix',                # CBOE Volatility Index\n",
    "        '^GSPC': 'sp500',             # S&P 500 Index\n",
    "        '^IRX': 't3m_yield',          # 13-Week T-Bill Yield (percent)\n",
    "        '^IXIC': 'nasdaq',            # NASDAQ Composite Index (optional)\n",
    "    }\n",
    "\n",
    "    end = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Download adjusted close prices for all tickers\n",
    "    data = yf.download(list(tickers.keys()), start='2007-07-30', end=end, progress=False)\n",
    "    data = data['Close']\n",
    "    data = data.rename(columns=tickers)\n",
    "\n",
    "    # Compute S&P 500 realized volatility (20-day rolling sum of squared log returns, annualized)\n",
    "    if 'sp500' in data.columns:\n",
    "        sp500_logret = np.log(data['sp500'] / data['sp500'].shift(1))\n",
    "        sp500_sqret = sp500_logret ** 2 # Squared log returns\n",
    "        data['sp500_volatility'] = np.sqrt(sp500_sqret.rolling(window=20, min_periods=1).sum() * 252) # Annualized volatility\n",
    "\n",
    "    # Handle missing values (forward-fill across non-trading days)\n",
    "    data = data.ffill().dropna()    \n",
    "\n",
    "    df = data.copy()\n",
    "\n",
    "    metadata = {\n",
    "        'wti_price': {'frequency': 'daily', 'source': 'Yahoo Finance', 'publication_lag': '0D'},\n",
    "        'brent_price': {'frequency': 'daily', 'source': 'Yahoo Finance', 'publication_lag': '0D'},\n",
    "        'gold_price': {'frequency': 'daily', 'source': 'Yahoo Finance', 'publication_lag': '0D'},\n",
    "        'copper_price': {'frequency': 'daily', 'source': 'Yahoo Finance', 'publication_lag': '0D'},\n",
    "        'dxy': {'frequency': 'daily', 'source': 'Yahoo Finance', 'publication_lag': '0D'},\n",
    "        '10y_yield': {'frequency': 'daily', 'source': 'Yahoo Finance', 'publication_lag': '0D'},\n",
    "        't3m_yield': {'frequency': 'daily', 'source': 'Yahoo Finance', 'publication_lag': '0D'},\n",
    "        'vix': {'frequency': 'daily', 'source': 'Yahoo Finance', 'publication_lag': '0D'},\n",
    "        'sp500': {'frequency': 'daily', 'source': 'Yahoo Finance', 'publication_lag': '0D'},\n",
    "        'sp500_volatility': {'frequency': 'daily', 'source': 'Computed from ^GSPC', 'publication_lag': '0D'},\n",
    "        'nasdaq': {'frequency': 'daily', 'source': 'Yahoo Finance', 'publication_lag': '0D'},\n",
    "    }\n",
    "\n",
    "    print(\"Data ingestion complete.\")\n",
    "    return df, metadata\n",
    "\n",
    "# --- Conflict & UCDP helpers ---\n",
    "KEY_ACTORS = {\n",
    "    'United States of America', 'Yemen (North Yemen)', 'United Arab Emirates', 'Saudi Arabia', 'Russia',\n",
    "    'Iran', 'Iraq', 'Egypt', 'Oman', 'Kuwait'\n",
    "}\n",
    "sides = [\n",
    "    \"Government of Iran\", \"Government of Iraq\", \"Government of Russia (Soviet Union)\",\n",
    "    \"Government of Kuwait\", \"Libya Dawn\", \"LNA\", \"Forces of Khalifa al-Ghawil\",\n",
    "    \"February 17 Martyrs Brigade\", \"Rafallah al-Sahati Brigade\", \"Qadhadhfa\",\n",
    "    \"Mashashia\", \"Awlad Suleiman\", \"Awlad Zeid\", \"Ajdabiya Revolutionaries Shura Council\",\n",
    "    \"Government of Yemen (North Yemen)\", \"Forces of the Presidential Leadership Council\",\n",
    "    \"Democratic Republic of Yemen\", \"Islah\", \"Hezbollah\", \"Government of Syria\",\n",
    "    \"Government of United States of America\"\n",
    "]\n",
    "KEY_ACTORS_LOWER = {x.strip().lower() for x in KEY_ACTORS}\n",
    "SIDES_SET_LOWER = {x.strip().lower() for x in sides}\n",
    "\n",
    "def _find_column(df: pd.DataFrame, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns: return c\n",
    "    lower_map = {c.lower(): c for c in df.columns}\n",
    "    for c in candidates:\n",
    "        if c.lower() in lower_map: return lower_map[c.lower()]\n",
    "    return None\n",
    "\n",
    "def _normalize_text(series: pd.Series) -> pd.Series:\n",
    "    if series is None: return pd.Series([], dtype='object')\n",
    "    return series.fillna('').astype(str).str.strip().str.lower()\n",
    "\n",
    "def _series_in_set(series: pd.Series, values_lower: set) -> pd.Series:\n",
    "    s = _normalize_text(series)\n",
    "    return s.isin(values_lower)\n",
    "\n",
    "def _read_zip_select(zip_path: str, prefer_contains=None) -> pd.DataFrame:\n",
    "    with zipfile.ZipFile(zip_path) as z:\n",
    "        csv_names = [n for n in z.namelist() if n.lower().endswith('.csv')]\n",
    "        if not csv_names: raise ValueError(f\"No CSV files found in {zip_path}\")\n",
    "        target = csv_names[0]\n",
    "        if prefer_contains:\n",
    "            for pat in prefer_contains:\n",
    "                matches = [n for n in csv_names if pat.lower() in n.lower()]\n",
    "                if matches:\n",
    "                    target = matches[0]\n",
    "                    break\n",
    "        with z.open(target) as f:\n",
    "            df = pd.read_csv(f, low_memory=False)\n",
    "    \n",
    "    rename_map = {\n",
    "        _find_column(df, ['Year', 'year']): 'Year',\n",
    "        _find_column(df, ['Country', 'country']): 'Country',\n",
    "        _find_column(df, ['side_a', 'SideA', 'Side A']): 'side_a',\n",
    "        _find_column(df, ['side_b', 'SideB', 'Side B']): 'side_b',\n",
    "        _find_column(df, ['best', 'bd_best', 'Best', 'BEST']): 'best',\n",
    "    }\n",
    "    rename_map = {k: v for k, v in rename_map.items() if k is not None}\n",
    "    df = df[list(rename_map.keys())].rename(columns=rename_map)\n",
    "\n",
    "    if 'Year' in df.columns: df['Year'] = pd.to_numeric(df['Year'], errors='coerce').astype('Int64')\n",
    "    if 'best' in df.columns: df['best'] = pd.to_numeric(df['best'], errors='coerce')\n",
    "    for col in ['Country', 'side_a', 'side_b']:\n",
    "        if col in df.columns: df[col] = df[col].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "def load_conflict_sources(ucdp_zip_path: str = 'ucdp-brd-conf-251-csv.zip', ged_zip_path: str = 'ged251-csv.zip'):\n",
    "    brd = _read_zip_select(ucdp_zip_path, prefer_contains=['BattleDeaths', 'conf'])\n",
    "    if not brd.empty:\n",
    "        country_match = _series_in_set(brd.get('Country'), KEY_ACTORS_LOWER)\n",
    "        sa_match = _series_in_set(brd.get('side_a'), SIDES_SET_LOWER)\n",
    "        sb_match = _series_in_set(brd.get('side_b'), SIDES_SET_LOWER)\n",
    "        brd['country_flag'] = country_match.astype('Int64')\n",
    "        brd['side_a_flag'] = sa_match.astype('Int64')\n",
    "        brd['side_b_flag'] = sb_match.astype('Int64')\n",
    "        brd['flag'] = brd[['country_flag', 'side_a_flag', 'side_b_flag']].max(axis=1).astype('Int64')\n",
    "        brd = brd.drop(columns=['country_flag', 'side_a_flag', 'side_b_flag'], errors='ignore')\n",
    "\n",
    "    ged = _read_zip_select(ged_zip_path, prefer_contains=['ged', 'GED'])\n",
    "    if not ged.empty:\n",
    "        country_match = _series_in_set(ged.get('Country'), KEY_ACTORS_LOWER)\n",
    "        sa_match = _series_in_set(ged.get('side_a'), SIDES_SET_LOWER)\n",
    "        sb_match = _series_in_set(ged.get('side_b'), SIDES_SET_LOWER)\n",
    "        ged['country_flag'] = country_match.astype('Int64')\n",
    "        ged['side_a_flag'] = sa_match.astype('Int64')\n",
    "        ged['side_b_flag'] = sb_match.astype('Int64')\n",
    "        ged['flag'] = ged[['country_flag', 'side_a_flag', 'side_b_flag']].max(axis=1).astype('Int64')\n",
    "        ged = ged.drop(columns=['country_flag', 'side_a_flag', 'side_b_flag'], errors='ignore')\n",
    "\n",
    "    merged_raw = pd.concat([brd, ged], ignore_index=True, sort=False)\n",
    "\n",
    "    def _agg_yearly(df, src):\n",
    "        if df.empty: return pd.DataFrame(columns=['Year'])\n",
    "        g = df.groupby('Year', dropna=True).agg(\n",
    "            best=('best', 'sum'),\n",
    "            flag=('flag', 'max')\n",
    "        ).reset_index()\n",
    "        return g.rename(columns=lambda c: f'{src}_{c}' if c != 'Year' else c)\n",
    "\n",
    "    brd_yearly = _agg_yearly(brd, 'brd')\n",
    "    ged_yearly = _agg_yearly(ged, 'ged')\n",
    "\n",
    "    merged_yearly = pd.merge(brd_yearly, ged_yearly, on='Year', how='outer').sort_values('Year').fillna(0)\n",
    "    merged_yearly['Year'] = merged_yearly['Year'].astype('Int64')\n",
    "    \n",
    "    return merged_raw, merged_yearly\n",
    "\n",
    "def _prepare_annual_features(merged_yearly: pd.DataFrame) -> pd.DataFrame:\n",
    "    if merged_yearly is None or merged_yearly.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = merged_yearly.copy().set_index('Year')\n",
    "    for col in ['brd_best', 'ged_best']:\n",
    "        if col not in df.columns: df[col] = 0\n",
    "\n",
    "    df['total_best'] = df[['brd_best', 'ged_best']].sum(axis=1)\n",
    "\n",
    "    # Single unified flag across sources\n",
    "    if 'brd_flag' not in df.columns: df['brd_flag'] = 0\n",
    "    if 'ged_flag' not in df.columns: df['ged_flag'] = 0\n",
    "    df['flag'] = df[['brd_flag', 'ged_flag']].max(axis=1).astype('Int64')\n",
    "\n",
    "    df['yoy_diff_total_best'] = df['total_best'].diff()\n",
    "    df['yoy_pct_total_best'] = df['total_best'].pct_change().replace([np.inf, -np.inf], 0)\n",
    "    df['rolling_mean_3y_total_best'] = df['total_best'].rolling(window=3, min_periods=1).mean()\n",
    "    df['rolling_std_3y_total_best'] = df['total_best'].rolling(window=3, min_periods=1).std()\n",
    "    df['lag1_total_best'] = df['total_best'].shift(1)\n",
    "    df['lag2_total_best'] = df['total_best'].shift(2)\n",
    "\n",
    "    keep_cols = [\n",
    "        'flag',\n",
    "        'total_best', 'yoy_diff_total_best', 'yoy_pct_total_best',\n",
    "        'rolling_mean_3y_total_best', 'rolling_std_3y_total_best',\n",
    "        'lag1_total_best', 'lag2_total_best'\n",
    "    ]\n",
    "    return df[keep_cols].fillna(0)\n",
    "\n",
    "def _annual_to_published_daily(annual_df: pd.DataFrame, df_daily_index: pd.DatetimeIndex, publication_lag_months: int = 12) -> pd.DataFrame:\n",
    "    if annual_df is None or annual_df.empty:\n",
    "        return pd.DataFrame(index=df_daily_index)\n",
    "\n",
    "    availability_dates = pd.to_datetime(annual_df.index.astype(str)) + pd.DateOffset(years=1) + pd.DateOffset(months=publication_lag_months-12)\n",
    "    pub_df = annual_df.copy()\n",
    "    pub_df.index = availability_dates\n",
    "\n",
    "    full_index = df_daily_index.union(pub_df.index).sort_values()\n",
    "    daily_pub = pub_df.reindex(full_index).ffill().reindex(df_daily_index)\n",
    "    return daily_pub\n",
    "\n",
    "def merge_conflict_features_with_daily(df_daily: pd.DataFrame, merged_yearly: pd.DataFrame, publication_lag_months: int = 12, metadata: Optional[Dict] = None) -> Tuple[pd.DataFrame, Dict]:\n",
    "    if metadata is None: metadata = {}\n",
    "    if df_daily is None or df_daily.empty: raise ValueError(\"df_daily must be a non-empty DataFrame\")\n",
    "    if not isinstance(df_daily.index, pd.DatetimeIndex): raise ValueError(\"df_daily.index must be a pandas.DatetimeIndex\")\n",
    "\n",
    "    annual_feats = _prepare_annual_features(merged_yearly)\n",
    "    if annual_feats.empty:\n",
    "        return df_daily.copy(), metadata\n",
    "\n",
    "    daily_feature_df = _annual_to_published_daily(annual_feats, df_daily.index, publication_lag_months=publication_lag_months)\n",
    "    out = df_daily.merge(daily_feature_df, how='left', left_index=True, right_index=True)\n",
    "\n",
    "    new_meta = {c: {'frequency': 'annual->daily (published, ffilled)', 'source': 'UCDP', 'publication_lag_months': publication_lag_months} for c in daily_feature_df.columns}\n",
    "    combined_meta = {**metadata, **new_meta}\n",
    "    return out, combined_meta\n",
    "\n",
    "def add_gpr_features(df_daily: pd.DataFrame, gpr_daily_path: str, gpr_monthly_path: str, country_list=None) -> pd.DataFrame:\n",
    "    df = df_daily.copy()\n",
    "\n",
    "    # --- Load daily GPR data ---\n",
    "    gpr_daily = pd.read_excel(gpr_daily_path).rename(columns={'yyyymmdd': 'date'})\n",
    "    gpr_daily['date'] = pd.to_datetime(gpr_daily['date'], format='%Y%m%d')\n",
    "    gpr_daily = gpr_daily.set_index('date').sort_index()\n",
    "    daily_features = ['GPRD', 'GPRD_ACT', 'GPRD_THREAT', 'GPRD_MA7', 'GPRD_MA30']\n",
    "    df = df.merge(gpr_daily[daily_features], how='left', left_index=True, right_index=True)\n",
    "\n",
    "    # --- Load monthly GPR data ---\n",
    "    gpr_monthly = pd.read_excel(gpr_monthly_path)\n",
    "    gpr_monthly['month'] = pd.to_datetime(gpr_monthly['month'])\n",
    "    gpr_monthly = gpr_monthly.set_index('month').sort_index()\n",
    "    monthly_features = ['GPR', 'GPRT', 'GPRA']\n",
    "    if country_list:\n",
    "        for c in country_list:\n",
    "            col_name = f'GPRC_{c.upper()[:3]}'\n",
    "            if col_name in gpr_monthly.columns:\n",
    "                monthly_features.append(col_name)\n",
    "    \n",
    "    # Forward-fill monthly data to daily frequency\n",
    "    gpr_monthly_daily = gpr_monthly[monthly_features].reindex(df.index, method='ffill')\n",
    "    df = df.merge(gpr_monthly_daily, how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_daily_epu(df_daily: pd.DataFrame, epu_path: str, lag: int = 1) -> pd.DataFrame:\n",
    "    df = df_daily.copy()\n",
    "    epu = pd.read_csv(epu_path)\n",
    "    epu['date'] = pd.to_datetime(epu[['year', 'month', 'day']])\n",
    "    epu = epu.rename(columns={'daily_policy_index': 'EPU_index'})\n",
    "    epu = epu[['date', 'EPU_index']].set_index('date').sort_index()\n",
    "\n",
    "    if lag > 0:\n",
    "        epu['EPU_index'] = epu['EPU_index'].shift(lag)\n",
    "\n",
    "    df = df.merge(epu, how='left', left_index=True, right_index=True)\n",
    "    return df\n",
    "\n",
    "def add_bdi_prices(df_daily: pd.DataFrame, bdi_path: str, lag: int = 1, date_column: Optional[str] = None) -> pd.DataFrame:\n",
    "    df = df_daily.copy()\n",
    "    if bdi_path.lower().endswith('.csv'):\n",
    "        bdi = pd.read_csv(bdi_path)\n",
    "    else:\n",
    "        bdi = pd.read_excel(bdi_path)\n",
    "\n",
    "    def _find_date_col(frame):\n",
    "        candidates = ['Date', 'date', 'As of Date', 'timestamp']\n",
    "        for c in candidates:\n",
    "            if c in frame.columns: return c\n",
    "        raise ValueError(\"No date column found in BDI file.\")\n",
    "    \n",
    "    date_col = date_column or _find_date_col(bdi)\n",
    "    bdi['date'] = pd.to_datetime(bdi[date_col])\n",
    "    bdi = bdi.set_index('date').sort_index()\n",
    "\n",
    "    rename_map = {c: f\"BDIY {c.split()[-1]}\" for c in bdi.columns if isinstance(c, str)}\n",
    "    bdi = bdi.rename(columns=rename_map)\n",
    "\n",
    "    # Reindex to match the main df's date range and ffill\n",
    "    bdi_daily = bdi.reindex(df.index).ffill()\n",
    "\n",
    "    if lag > 0:\n",
    "        bdi_daily = bdi_daily.shift(lag)\n",
    "\n",
    "    return df.merge(bdi_daily, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# --- Weather helpers ---\n",
    "city_coords = {\n",
    "    'Houston': (29.7604, -95.3698), 'Dallas': (32.7767, -96.7970),\n",
    "    'Denver': (39.7392, -104.9903), 'New York': (40.7128, -74.0060),\n",
    "    'Los Angeles': (34.0522, -118.2437), 'Riyadh': (24.7136, 46.6753),\n",
    "    'London': (51.5074, -0.1278)\n",
    "}\n",
    "\n",
    "def fetch_weather_data(city_name, lat, lon, start_date, end_date, timezone=\"UTC\"):\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\"latitude\": lat, \"longitude\": lon, \"start_date\": start_date, \"end_date\": end_date, \"daily\": [\"temperature_2m_mean\", \"precipitation_sum\", \"wind_speed_10m_max\"], \"timezone\": timezone}\n",
    "    resp = requests.get(url, params=params)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    if \"daily\" not in data: return pd.DataFrame()\n",
    "    df = pd.DataFrame(data[\"daily\"])\n",
    "    df[\"city\"] = city_name\n",
    "    return df\n",
    "\n",
    "def fetch_city_weather_batched(city_name, lat, lon, start_date, end_date, chunk_days=365, sleep_sec=0.2):\n",
    "    frames = []\n",
    "    current_start = pd.to_datetime(start_date)\n",
    "    end_dt = pd.to_datetime(end_date)\n",
    "    while current_start <= end_dt:\n",
    "        current_end = current_start + pd.Timedelta(days=chunk_days - 1)\n",
    "        if current_end > end_dt:\n",
    "            current_end = end_dt\n",
    "        df_part = fetch_weather_data(city_name, lat, lon, current_start.strftime('%Y-%m-%d'), current_end.strftime('%Y-%m-%d'))\n",
    "        if not df_part.empty:\n",
    "            frames.append(df_part)\n",
    "        time.sleep(sleep_sec)\n",
    "        current_start += pd.Timedelta(days=chunk_days)\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "def get_weather_data_for_analysis(start_date, end_date, cities=None):\n",
    "    target_cities = cities or list(city_coords.keys())\n",
    "    frames = [fetch_city_weather_batched(c, *city_coords[c], start_date, end_date) for c in target_cities]\n",
    "    if not any(not df.empty for df in frames): return pd.DataFrame()\n",
    "    \n",
    "    out = pd.concat(frames, ignore_index=True).rename(columns={'time': 'date', 'temperature_2m_mean': 'temp_mean_c', 'precipitation_sum': 'precip_mm', 'wind_speed_10m_max': 'wind_max_ms'})\n",
    "    out['date'] = pd.to_datetime(out['date'])\n",
    "    return out\n",
    "\n",
    "def integrate_weather_with_oil_data(oil_data: pd.DataFrame, weather_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    if weather_data.empty:\n",
    "        print(\"No weather data to integrate\")\n",
    "        return oil_data\n",
    "    \n",
    "    weather_daily = weather_data.groupby('date').agg(\n",
    "        temp_mean_c_mean=('temp_mean_c', 'mean'), temp_mean_c_std=('temp_mean_c', 'std'),\n",
    "        temp_mean_c_min=('temp_mean_c', 'min'), temp_mean_c_max=('temp_mean_c', 'max'),\n",
    "        precip_mm_sum=('precip_mm', 'sum'), precip_mm_max=('precip_mm', 'max'),\n",
    "        wind_max_ms_mean=('wind_max_ms', 'mean'), wind_max_ms_max=('wind_max_ms', 'max')\n",
    "    )\n",
    "    return oil_data.merge(weather_daily, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# --- Analysis helpers ---\n",
    "def plot_stationarity_check(series, series_name=''):\n",
    "    \"\"\"\n",
    "    Plots a series, its rolling stats, and runs the ADF test.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    ax.plot(series, label='Original Series', color='blue', alpha=0.8)\n",
    "\n",
    "    # Rolling statistics (using 1-year window)\n",
    "    rolling_mean = series.rolling(window=252).mean()\n",
    "    rolling_std = series.rolling(window=252).std()\n",
    "    ax.plot(rolling_mean, color='red', label='Rolling Mean (252-day)')\n",
    "    ax.plot(rolling_std, color='black', label='Rolling Std Dev (252-day)')\n",
    "\n",
    "    # ADF Test\n",
    "    result = adfuller(series.dropna())\n",
    "    p_value = result[1]\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_title(\n",
    "        f'Stationarity Check for {series_name}\\nADF p-value: {p_value:.4f}',\n",
    "        fontsize=16,\n",
    "    )\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "def plot_cross_correlation(df, target_col, feature_col, max_lag=20,\n",
    "                           window_size=252):\n",
    "    \"\"\"\n",
    "    Plots the cross-correlation between a target and a feature for various\n",
    "    lags. Calculates significance using a simple heuristic (2/sqrt(N)).\n",
    "    \"\"\"\n",
    "    lags = np.arange(-max_lag, max_lag + 1)\n",
    "\n",
    "    # Ensure no NaNs for correlation calculation within the rolling window\n",
    "    temp_df = df[[target_col, feature_col]].dropna()\n",
    "    if temp_df.empty:\n",
    "        print(\n",
    "            f\"Skipping cross-correlation for {target_col} and {feature_col} \"\n",
    "            \"due to empty data after dropna.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    corrs = [\n",
    "        temp_df[target_col].corr(temp_df[feature_col].shift(lag)) for lag in lags\n",
    "    ]\n",
    "\n",
    "    # Calculate confidence interval (approx. 95%)\n",
    "    # For N samples, standard error is 1/sqrt(N-1). Approx 2*SE for 95% CI.\n",
    "    # N is the number of effective observations after dropping NaNs and\n",
    "    # shifting.\n",
    "    N = len(temp_df) - max_lag  # Conservative estimate\n",
    "    if N > 1:\n",
    "        confidence_bound = 2 / np.sqrt(N)\n",
    "    else:\n",
    "        confidence_bound = 0  # No confidence if too few samples\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    ax.stem(lags, corrs, basefmt=\"C7--\")\n",
    "\n",
    "    ax.axhline(confidence_bound, color='red', linestyle='--', lw=1,\n",
    "               label=f'95% CI (approx)')\n",
    "    ax.axhline(-confidence_bound, color='red', linestyle='--', lw=1)\n",
    "    ax.axvline(0, color='black', linestyle=':', lw=1)  # Lag 0 line\n",
    "\n",
    "    ax.set_title(\n",
    "        f'Cross-Correlation: {target_col} vs. Lagged {feature_col}',\n",
    "        fontsize=16,\n",
    "    )\n",
    "    ax.set_xlabel(\n",
    "        f'Lag (days, positive means {feature_col} lags {target_col})',\n",
    "        fontsize=12,\n",
    "    )\n",
    "    ax.set_ylabel('Correlation Coefficient', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data fetching & initial ingestion ---\n",
    "raw_data, metadata = ingest_data_tickers()\n",
    "print(\"Raw Data Head:\")\n",
    "print(raw_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initial visualization of core market drivers (raw levels, normalized) ---\n",
    "print(\"\\n--- Visualizing Initial Raw Market Data ---\")\n",
    "# Select key series for initial overview (raw levels)\n",
    "key_series_initial = ['wti_price', 'brent_price', 'sp500', 'dxy', 'vix', 'sp500_volatility']\n",
    "df_plot_initial = raw_data[key_series_initial].dropna()\n",
    "\n",
    "# Normalize to 100 at start for comparability\n",
    "df_plot_initial_normalized = (df_plot_initial / df_plot_initial.iloc[0]) * 100\n",
    "\n",
    "plt.figure(figsize=(18, 10))\n",
    "for col in df_plot_initial_normalized.columns:\n",
    "    plt.plot(df_plot_initial_normalized.index, df_plot_initial_normalized[col], label=col, linewidth=1.5)\n",
    "\n",
    "plt.title('Normalized Time Series: Core Market Drivers (Raw Levels)', fontsize=18)\n",
    "plt.ylabel('Normalized Value (Start=100)', fontsize=14)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=12)\n",
    "plt.grid(True, alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Distribution of S&P 500 Realized Volatility ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(raw_data['sp500_volatility'].dropna(), kde=True, bins=50, color='purple')\n",
    "plt.title('Distribution of S&P 500 Realized Volatility', fontsize=16)\n",
    "plt.xlabel('Annualized Volatility', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.grid(True, alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Conflict data ingestion and merging with daily data ---\n",
    "merged_raw, merged_yearly = load_conflict_sources('ucdp-brd-conf-251-csv.zip', 'ged251-csv.zip')\n",
    "raw_data_aug, metadata = merge_conflict_features_with_daily(raw_data, merged_yearly, publication_lag_months=12, metadata=metadata)\n",
    "print(\"\\nDataFrame after adding conflict features (with single 'flag' column):\")\n",
    "print(raw_data_aug.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualizing Conflict Features with Unified 'flag' ---\n",
    "print(\"\\n--- Visualizing Conflict Features with Unified 'flag' ---\")\n",
    "fig, axes = plt.subplots(3, 1, figsize=(18, 15), sharex=True)\n",
    "\n",
    "# Plot 1: Total Conflict Deaths and its Year-Over-Year change (No change here)\n",
    "axes[0].plot(raw_data_aug.index, raw_data_aug['total_best'], label='Total Conflict Deaths (Annual F-fill)', color='red', alpha=0.7)\n",
    "axes[0].plot(raw_data_aug.index, raw_data_aug['yoy_diff_total_best'], label='YOY Diff Total Conflict Deaths', color='orange', alpha=0.7, linestyle='--')\n",
    "axes[0].set_title('Geopolitical Conflict Intensity (UCDP BRD/GED)', fontsize=16)\n",
    "axes[0].set_ylabel('Annual Deaths / Change', fontsize=12)\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].grid(True, alpha=0.5)\n",
    "\n",
    "# Plot 2: Rolling Statistics of Conflict Intensity (No change here)\n",
    "axes[1].plot(raw_data_aug.index, raw_data_aug['rolling_mean_3y_total_best'], label='3-Year Rolling Mean', color='darkblue')\n",
    "axes[1].plot(raw_data_aug.index, raw_data_aug['rolling_std_3y_total_best'], label='3-Year Rolling Std', color='cyan')\n",
    "axes[1].set_title('Conflict Intensity: Rolling Statistics', fontsize=16)\n",
    "axes[1].set_ylabel('Value', fontsize=12)\n",
    "axes[1].legend(loc='upper left')\n",
    "axes[1].grid(True, alpha=0.5)\n",
    "\n",
    "# Plot 3: The Unified Key Actor Involvement Flag (THIS IS THE ALTERED PART)\n",
    "# We now use the single 'flag' column instead of 'country_flag'.\n",
    "axes[2].fill_between(raw_data_aug.index, 0, raw_data_aug['flag'], color='green', alpha=0.3, label='Key Actor Involved')\n",
    "axes[2].set_title('Unified Key Actor Involvement Flag (1=Active)', fontsize=16)\n",
    "axes[2].set_ylabel('Flag (0/1)', fontsize=12)\n",
    "axes[2].set_yticks([0, 1])\n",
    "axes[2].legend(loc='upper left')\n",
    "axes[2].grid(True, alpha=0.5)\n",
    "\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add GPR features (daily and monthly) ---\n",
    "def add_gpr_wrapper():\n",
    "    key_countries = ['USA', 'RUS', 'SAU', 'IRN', 'IRQ']\n",
    "    return add_gpr_features(raw_data_aug, \n",
    "                            gpr_daily_path='data_gpr_daily_recent.xls',\n",
    "                            gpr_monthly_path='data_gpr_export.xls',\n",
    "                            country_list=key_countries)\n",
    "\n",
    "raw_data_aug_with_gpr = add_gpr_wrapper()\n",
    "print(\"\\nDataFrame after adding GPR features:\")\n",
    "print(raw_data_aug_with_gpr.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualizing GPR features ---\n",
    "print(\"\\n--- Visualizing Geopolitical Risk (GPR) Features ---\")\n",
    "fig, axes = plt.subplots(3, 1, figsize=(18, 15), sharex=True)\n",
    "\n",
    "# Top panel: Daily GPR and moving averages\n",
    "axes[0].plot(raw_data_aug_with_gpr.index, raw_data_aug_with_gpr['GPRD'], label='GPRD (Daily)', color='blue', alpha=0.7)\n",
    "axes[0].plot(raw_data_aug_with_gpr.index, raw_data_aug_with_gpr['GPRD_MA7'], label='GPRD 7-day MA', color='orange', linestyle='--')\n",
    "axes[0].plot(raw_data_aug_with_gpr.index, raw_data_aug_with_gpr['GPRD_MA30'], label='GPRD 30-day MA', color='red', linestyle='--')\n",
    "axes[0].set_title('Daily GPR Index and Moving Averages', fontsize=16)\n",
    "axes[0].set_ylabel('Index Value', fontsize=12)\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].grid(True, alpha=0.5)\n",
    "\n",
    "# Middle panel: Threats vs Acts\n",
    "axes[1].plot(raw_data_aug_with_gpr.index, raw_data_aug_with_gpr['GPRD_THREAT'], label='GPRD_THREAT', color='green', alpha=0.7)\n",
    "axes[1].plot(raw_data_aug_with_gpr.index, raw_data_aug_with_gpr['GPRD_ACT'], label='GPRD_ACT', color='purple', alpha=0.7)\n",
    "axes[1].set_title('Daily GPR Threats vs. Acts', fontsize=16)\n",
    "axes[1].set_ylabel('Index Value', fontsize=12)\n",
    "axes[1].legend(loc='upper left')\n",
    "axes[1].grid(True, alpha=0.5)\n",
    "\n",
    "# Bottom panel: Country-specific GPR (up to 3 for clarity)\n",
    "gprc_cols = [col for col in raw_data_aug_with_gpr.columns if col.startswith('GPRC_')]\n",
    "for col in gprc_cols[:3]:\n",
    "    axes[2].plot(raw_data_aug_with_gpr.index, raw_data_aug_with_gpr[col], label=col, linewidth=1.5, alpha=0.8)\n",
    "axes[2].set_title('Country-Specific GPR Indices', fontsize=16)\n",
    "axes[2].set_ylabel('Index Value', fontsize=12)\n",
    "axes[2].legend(loc='upper left')\n",
    "axes[2].grid(True, alpha=0.5)\n",
    "\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add EPU features ---\n",
    "raw_data_aug_with_epu = add_daily_epu(raw_data_aug_with_gpr, epu_path='All_Daily_Policy_Data.csv', lag=1)\n",
    "print(\"\\nDataFrame after adding EPU features:\")\n",
    "print(raw_data_aug_with_epu.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize EPU ---\n",
    "print(\"\\n--- Visualizing Economic Policy Uncertainty (EPU) ---\")\n",
    "plt.figure(figsize=(18, 7))\n",
    "plt.plot(raw_data_aug_with_epu.index, raw_data_aug_with_epu['EPU_index'], label='EPU Index (Lagged)', color='darkgreen', alpha=0.8)\n",
    "plt.title('Economic Policy Uncertainty (EPU) Index', fontsize=18)\n",
    "plt.ylabel('Index Value', fontsize=14)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True, alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add BDI features ---\n",
    "raw_data_aug_with_bdi = add_bdi_prices(raw_data_aug_with_epu, bdi_path='koyfin_2025-09-15.csv', lag=1)\n",
    "print(\"\\nDataFrame after adding BDI features:\")\n",
    "print(raw_data_aug_with_bdi.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize BDI ---\n",
    "print(\"\\n--- Visualizing Baltic Dry Index (BDI) ---\")\n",
    "plt.figure(figsize=(18, 7))\n",
    "plt.plot(raw_data_aug_with_bdi.index, raw_data_aug_with_bdi['BDIY Close'], label='BDIY Close (Lagged)', color='brown', alpha=0.8)\n",
    "plt.title('Baltic Dry Index (BDIY) Close Price', fontsize=18)\n",
    "plt.ylabel('Index Value', fontsize=14)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True, alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fetch & integrate weather data ---\n",
    "start_date = raw_data_aug.index.min().strftime('%Y-%m-%d')\n",
    "end_date = raw_data_aug.index.max().strftime('%Y-%m-%d')\n",
    "cities_to_fetch = ['Houston', 'Dallas', 'New York', 'Riyadh', 'London']\n",
    "\n",
    "weather_data = get_weather_data_for_analysis(start_date, end_date, cities=cities_to_fetch)\n",
    "\n",
    "raw_data_final = pd.DataFrame()\n",
    "if not weather_data.empty:\n",
    "    raw_data_final = integrate_weather_with_oil_data(raw_data_aug_with_bdi, weather_data)\n",
    "    print(\"\\nWeather features added. Final DataFrame head:\")\n",
    "    print(raw_data_final.head())\n",
    "else:\n",
    "    print(\"\\nWeather data fetch failed. Proceeding without weather features.\")\n",
    "    raw_data_final = raw_data_aug_with_bdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NA checks and cleanup steps (no preprocessing changes applied) ---\n",
    "for column in raw_data_final.columns:\n",
    "    print(column, raw_data_final[column].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_final.ffill(inplace=True)\n",
    "columns_to_drop = ['BDIY Date', 'BDIY Open', 'BDIY High', 'BDIY Low']\n",
    "raw_data_final = raw_data_final.drop(columns=columns_to_drop, errors='ignore')\n",
    "raw_data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analysis: build df_analysis with engineered stationary features (no preprocessing beyond transformations) ---\n",
    "# Ensure raw_data_final is available from previous blocks\n",
    "if 'raw_data_final' not in locals():\n",
    "    print(\"WARNING: raw_data_final not found. Please ensure all previous blocks were executed correctly.\")\n",
    "    # Attempt to use raw_data_aug as a fallback if raw_data_final somehow wasn't created\n",
    "    raw_data_final = raw_data_aug.copy() \n",
    "\n",
    "df = raw_data_final.copy() # Use the fully merged dataframe\n",
    "\n",
    "df_analysis = pd.DataFrame(index=df.index)\n",
    "\n",
    "# 1. Create the Target Variable (y)\n",
    "df_analysis['wti_price_logret'] = np.log(df['wti_price']).diff()\n",
    "\n",
    "# 2. Create Engineered Spreads and their differences\n",
    "df_analysis['brent_wti_spread'] = df['brent_price'] - df['wti_price']\n",
    "df_analysis['brent_wti_spread_diff'] = df_analysis['brent_wti_spread'].diff() # Difference the spread itself\n",
    "\n",
    "df_analysis['yield_curve_10y_3m'] = df['10y_yield'] - df['t3m_yield']\n",
    "df_analysis['yield_curve_10y_3m_diff'] = df_analysis['yield_curve_10y_3m'].diff() # Difference the yield curve\n",
    "\n",
    "# 3. Create Stationary Features (X)\n",
    "# Log returns for price-like series\n",
    "for col in ['sp500', 'gold_price', 'copper_price', 'BDIY Close']:\n",
    "    if col in df.columns: # Check if original column exists\n",
    "        df_analysis[f'{col}_logret'] = np.log(df[col]).diff()\n",
    "\n",
    "# Differences for indices and rates\n",
    "for col in ['dxy', 'vix', 'GPRD', 'EPU_index']:\n",
    "    if col in df.columns: # Check if original column exists\n",
    "        df_analysis[f'{col}_diff'] = df[col].diff()\n",
    "\n",
    "# Keep Annual Conflict Features As-Is (they are already step-wise or represent change)\n",
    "# Fill NaNs with 0 where appropriate for these flags/differences if they didn't exist before\n",
    "conflict_cols_to_keep = [\n",
    "    'yoy_diff_total_best', 'rolling_mean_3y_total_best', 'rolling_std_3y_total_best', 'country_flag',\n",
    "    'side_a_flag', 'side_b_flag' # Keeping flags as they are binary indicators\n",
    "]\n",
    "for col in conflict_cols_to_keep:\n",
    "    if col in df.columns:\n",
    "        df_analysis[col] = df[col].fillna(0) # Fill 0 for non-existent conflict periods\n",
    "\n",
    "# Add GPRC (country-specific) features - differenced\n",
    "gprc_cols = [c for c in df.columns if 'GPRC_' in c]\n",
    "for col in gprc_cols:\n",
    "    df_analysis[f'{col}_diff'] = df[col].diff()\n",
    "\n",
    "# Add Weather Features (Differenced)\n",
    "# It's better to calculate HDD/CDD, but for now we'll difference the mean temp and other aggregates\n",
    "weather_agg_cols = [col for col in df.columns if 'temp_mean_c_' in col or 'precip_mm_' in col or 'wind_max_ms_' in col]\n",
    "for col in weather_agg_cols:\n",
    "    if col in df.columns:\n",
    "        df_analysis[f'{col}_diff'] = df[col].diff()\n",
    "\n",
    "# Drop any initial NaNs created by transformations\n",
    "df_analysis = df_analysis.dropna()\n",
    "\n",
    "print(\"\\nFinal df_analysis head with all stationary and engineered features:\")\n",
    "print(df_analysis.head())\n",
    "print(f\"df_analysis shape: {df_analysis.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stationarity checks & plotting ---\n",
    "print(\"\\n--- Performing Stationarity Checks on Key Transformed Features ---\")\n",
    "\n",
    "# Check WTI Log Returns (your target)\n",
    "plot_stationarity_check(df_analysis['wti_price_logret'], 'WTI Log Returns (Target)')\n",
    "\n",
    "# Check Brent-WTI Spread (new engineered feature)\n",
    "plot_stationarity_check(df_analysis['brent_wti_spread'], 'Brent-WTI Spread')\n",
    "plot_stationarity_check(df_analysis['brent_wti_spread_diff'], 'Brent-WTI Spread (Differenced)')\n",
    "\n",
    "# Check S&P 500 Log Returns\n",
    "plot_stationarity_check(df_analysis['sp500_logret'], 'S&P 500 Log Returns')\n",
    "\n",
    "# Check 10Y-3M Yield Curve (engineered spread)\n",
    "plot_stationarity_check(df_analysis['yield_curve_10y_3m'], '10Y-3M Yield Curve (Level)')\n",
    "plot_stationarity_check(df_analysis['yield_curve_10y_3m_diff'], '10Y-3M Yield Curve (Differenced)')\n",
    "\n",
    "# Check VIX Differences\n",
    "plot_stationarity_check(df_analysis['vix_diff'], 'VIX Differences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Correlation matrix and top correlations with target ---\n",
    "print(\"\\n--- Correlation Matrix of Stationary Features ---\")\n",
    "fig, ax = plt.subplots(figsize=(20, 18)) # Adjust size for readability\n",
    "sns.heatmap(\n",
    "    df_analysis.corr(method='spearman'), # Spearman is robust to non-normal distributions\n",
    "    annot=False, # Set to True if your screen/plot size allows for clear annotations\n",
    "    cmap='RdBu_r', # Red-Blue diverging colormap\n",
    "    center=0,      # Center the colormap at 0 for clear positive/negative\n",
    "    linewidths=.5,\n",
    "    fmt=\".2f\",\n",
    "    ax=ax,\n",
    "    cbar_kws={'label': 'Spearman Correlation Coefficient'}\n",
    ")\n",
    "ax.set_title('Spearman Correlation Matrix: All Stationary & Engineered Features', fontsize=20)\n",
    "plt.xticks(rotation=90, ha='right', fontsize=10)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bonus: Print high-correlations with target\n",
    "target_corrs = df_analysis.corr(method='spearman')['wti_price_logret'].sort_values(ascending=False)\n",
    "print(\"\\nTop correlations with WTI Log Returns:\")\n",
    "print(target_corrs.drop('wti_price_logret').head(10)) # Top 10 positive\n",
    "print(target_corrs.drop('wti_price_logret').tail(10)) # Top 10 negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Distribution analysis of selected stationary features ---\n",
    "print(\"\\n--- Distribution Analysis of Key Stationary Features ---\")\n",
    "from scipy import stats\n",
    "\n",
    "# Select a few representative stationary features for detailed distribution plots\n",
    "features_for_dist = [\n",
    "    'wti_price_logret', 'sp500_logret', 'vix_diff', 'brent_wti_spread_diff',\n",
    "    'yield_curve_10y_3m_diff', 'GPRD_diff', 'EPU_index_diff', 'temp_mean_c_mean_diff'\n",
    "]\n",
    "\n",
    "num_features = len(features_for_dist)\n",
    "fig = plt.figure(figsize=(num_features * 4, 10)) # Adjust figure size dynamically\n",
    "gs = fig.add_gridspec(2, num_features) # 2 rows for hist/qq, num_features columns\n",
    "\n",
    "for i, col in enumerate(features_for_dist):\n",
    "    if col in df_analysis.columns:\n",
    "        # Histogram\n",
    "        ax_hist = fig.add_subplot(gs[0, i])\n",
    "        sns.histplot(df_analysis[col].dropna(), kde=True, bins=50, ax=ax_hist, color='skyblue')\n",
    "        ax_hist.set_title(f'Hist: {col}', fontsize=12)\n",
    "        ax_hist.set_xlabel('')\n",
    "\n",
    "        # QQ Plot\n",
    "        ax_qq = fig.add_subplot(gs[1, i])\n",
    "        stats.probplot(df_analysis[col].dropna(), dist=\"norm\", plot=ax_qq)\n",
    "        ax_qq.set_title(f'QQ: {col}', fontsize=12)\n",
    "        ax_qq.set_xlabel('')\n",
    "        ax_qq.set_ylabel('')\n",
    "\n",
    "plt.suptitle('Distribution & QQ Plots of Key Stationary Features', fontsize=20, y=1.02)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rolling correlation analysis ---\n",
    "print(\"\\n--- Rolling Correlation Analysis ---\")\n",
    "# Example: WTI Log Returns vs. S&P 500 Log Returns\n",
    "rolling_corr_sp500 = df_analysis['wti_price_logret'].rolling(window=252).corr(df_analysis['sp500_logret'])\n",
    "\n",
    "# Example: WTI Log Returns vs. Brent-WTI Spread (differenced)\n",
    "rolling_corr_spread = df_analysis['wti_price_logret'].rolling(window=252).corr(df_analysis['brent_wti_spread_diff'])\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(18, 12), sharex=True)\n",
    "\n",
    "axes[0].plot(rolling_corr_sp500.index, rolling_corr_sp500, color='darkblue', label='WTI LogRet vs. SP500 LogRet')\n",
    "axes[0].axhline(0, color='grey', linestyle='--', lw=1)\n",
    "axes[0].set_title('252-Day Rolling Correlation: WTI Log Returns vs. S&P 500 Log Returns', fontsize=16)\n",
    "axes[0].set_ylabel('Correlation', fontsize=12)\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].grid(True, alpha=0.5)\n",
    "\n",
    "axes[1].plot(rolling_corr_spread.index, rolling_corr_spread, color='purple', label='WTI LogRet vs. Brent-WTI Spread Diff')\n",
    "axes[1].axhline(0, color='grey', linestyle='--', lw=1)\n",
    "axes[1].set_title('252-Day Rolling Correlation: WTI Log Returns vs. Brent-WTI Spread Differences', fontsize=16)\n",
    "axes[1].set_ylabel('Correlation', fontsize=12)\n",
    "axes[1].legend(loc='upper left')\n",
    "axes[1].grid(True, alpha=0.5)\n",
    "\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Lead-lag (cross-correlation) analysis among key features ---\n",
    "print(\"\\n--- Lead-Lag Analysis (Cross-Correlation) ---\")\n",
    "\n",
    "# --- Key Cross-Correlations with your Target (`wti_price_logret`) ---\n",
    "plot_cross_correlation(df_analysis, 'wti_price_logret', 'sp500_logret', max_lag=10)\n",
    "plot_cross_correlation(df_analysis, 'wti_price_logret', 'vix_diff', max_lag=10)\n",
    "plot_cross_correlation(df_analysis, 'wti_price_logret', 'dxy_diff', max_lag=10)\n",
    "plot_cross_correlation(df_analysis, 'wti_price_logret', 'brent_wti_spread_diff', max_lag=5) # Shorter lag for spread\n",
    "plot_cross_correlation(df_analysis, 'wti_price_logret', 'GPRD_diff', max_lag=10)\n",
    "plot_cross_correlation(df_analysis, 'wti_price_logret', 'EPU_index_diff', max_lag=10)\n",
    "plot_cross_correlation(df_analysis, 'wti_price_logret', 'yield_curve_10y_3m_diff', max_lag=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature-target scatter plots ---\n",
    "print(\"\\n--- Feature-Target Scatter Plots ---\")\n",
    "\n",
    "# Choose a few top correlated features from your correlation matrix\n",
    "# Ensure they are stationary features from df_analysis\n",
    "features_for_scatter = [\n",
    "    'sp500_logret', 'brent_wti_spread_diff', 'dxy_diff', 'vix_diff'\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(features_for_scatter), figsize=(len(features_for_scatter) * 6, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(features_for_scatter):\n",
    "    if col in df_analysis.columns:\n",
    "        sns.scatterplot(x=df_analysis[col], y=df_analysis['wti_price_logret'], ax=axes[i], alpha=0.6, color='darkgreen')\n",
    "        axes[i].set_title(f'{col} vs. WTI Log Returns', fontsize=14)\n",
    "        axes[i].set_xlabel(col, fontsize=12)\n",
    "        if i == 0:\n",
    "            axes[i].set_ylabel('WTI Log Returns', fontsize=12)\n",
    "        else:\n",
    "            axes[i].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Seasonal decomposition (if temp data exists) ---\n",
    "print(\"\\n--- Seasonal Decomposition of Select Features ---\")\n",
    "# Decompose a feature that *might* have seasonality (e.g., raw mean temperature)\n",
    "# Note: Decomposition is typically for non-stationary series.\n",
    "# Here, we will use the raw mean temperature (if available from 'df')\n",
    "\n",
    "# You'll likely need to re-fetch/use the raw mean temperature data for this.\n",
    "# Let's assume you have 'temp_mean_c_mean' available from the original `raw_data_final`\n",
    "if 'temp_mean_c_mean' in raw_data_final.columns:\n",
    "    # Resample to weekly or monthly if daily data is too noisy for clear seasonality\n",
    "    # Or, apply to the raw daily data if it's clean enough\n",
    "    temp_series = raw_data_final['temp_mean_c_mean'].dropna()\n",
    "\n",
    "    if len(temp_series) > 2 * 365: # Need at least two full years for seasonal decomposition\n",
    "        try:\n",
    "            decomposition = seasonal_decompose(temp_series, model='additive', period=365) # Daily seasonality for weather\n",
    "\n",
    "            fig = decomposition.plot()\n",
    "            fig.set_size_inches(16, 10)\n",
    "            fig.suptitle('Seasonal Decomposition of Daily Mean Temperature', fontsize=18, y=1.02)\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
    "            plt.show()\n",
    "        except ValueError as e:\n",
    "            print(f\"Could not perform seasonal decomposition on temp_mean_c_mean: {e}\")\n",
    "            print(\"Often means data is too short or not regularly sampled for the period.\")\n",
    "    else:\n",
    "        print(\"Not enough data for meaningful seasonal decomposition of temperature (requires > 2 years).\")\n",
    "else:\n",
    "    print(\"Raw 'temp_mean_c_mean' not found for seasonal decomposition.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
